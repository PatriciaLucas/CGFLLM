{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": ""
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11654772,
          "sourceType": "datasetVersion",
          "datasetId": 7314122
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"margin: auto; background-color: white;\">\n",
        "    <tr>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Z-B119x0wOMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Causal Graph Fuzzy LLM**\n",
        "\n",
        "\n",
        "<td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1igs_5aIACphsNn2oZiE10-nRFvnSXv-K' alt=\"drawing\" width=\"2000\" />\n",
        "</td>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oumWebICweUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet git+https://github.com/PatriciaLucas/AutoML.git\n",
        "\n",
        "!pip install --quiet git+https://petroniocandido/clshq_tk.git\n",
        "\n",
        "!git clone https://github.com/PatriciaLucas/Fuzzy-Causal-LLM.git"
      ],
      "metadata": {
        "trusted": true,
        "id": "LD2wUQCunCM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ccc664-8c29-47ab-b60d-b37edeb3f5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "fatal: destination path 'Fuzzy-Causal-LLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "from AUTODCETS import util, feature_selection, datasets, save_database as sd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import clshq_tk\n",
        "from clshq_tk.modules.fuzzy import GridPartitioner, trimf, gaussmf, training_loop\n",
        "from clshq_tk.data.regression import RegressionTS\n",
        "from clshq_tk.common import DEVICE, DEFAULT_PATH, resume, checkpoint, order_window\n",
        "\n",
        "import sys\n",
        "sys.path.append('Fuzzy-Causal-LLM')\n",
        "import fuzzy_causal_text as fcllm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T20:30:28.095125Z",
          "iopub.execute_input": "2025-05-04T20:30:28.095708Z",
          "iopub.status.idle": "2025-05-04T20:30:55.634862Z",
          "shell.execute_reply.started": "2025-05-04T20:30:28.095675Z",
          "shell.execute_reply": "2025-05-04T20:30:55.634078Z"
        },
        "id": "_Gf0bm1bnCM3"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Datasets**:\n",
        "\n",
        "**1.   CLIMATIC_1:** SONDA --- TARGET: glo_avg --- 35.000 SAMPLES --- 12 VARIABLES\n",
        "\n",
        "**2.   ENERGY_1**: WIND ENERGY PRODUCTION --- TARGET: Power --- 43.800 SAMPLES --- 9 VARIABLES\n",
        "\n",
        "**3.   IOT_1:** HOUSEHOLD ELECTRICITY CONSUMPTION IN MEXICO --- TARGET: active_power --- 100.000 SAMPLES --- 14 VARIABLES\n",
        "\n",
        "**4.   ECONOMICS_1:** BITCOIN --- TARGET: AVG --- 2.970 SAMPLES --- 06 VARIABLES\n",
        "\n"
      ],
      "metadata": {
        "id": "RvVR3cAJvOsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_dataset = 'ECONOMICS_1'\n",
        "target = 'AVG'\n",
        "df = fcllm.upload_dataset(name_dataset)\n",
        "\n",
        "if name_dataset == 'CLIMATIC_1':\n",
        "  df = df.head(35000)\n",
        "elif name_dataset == 'ENERGY_1':\n",
        "  df = df.head(43800)\n",
        "elif name_dataset == 'IOT_1':\n",
        "  df = df.head(100000)\n",
        "else:\n",
        "  df = df.head(2970)\n",
        "\n",
        "max_lags = 20\n",
        "partitions = 30\n",
        "path_model = 'model'\n",
        "database_path = 'database_text.db'\n",
        "epochs = 20\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "name_model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_model)\n",
        "\n",
        "windows = fcllm.rolling_window(df, 10)\n",
        "\n",
        "# sd.execute(\"CREATE TABLE IF NOT EXISTS results(name_dataset TEXT, window INT, predict FLOAT, \\\n",
        "#                real FLOAT)\", database_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T23:37:11.594286Z",
          "iopub.execute_input": "2025-05-04T23:37:11.594648Z",
          "iopub.status.idle": "2025-05-04T23:37:13.663002Z",
          "shell.execute_reply.started": "2025-05-04T23:37:11.594624Z",
          "shell.execute_reply": "2025-05-04T23:37:13.662333Z"
        },
        "id": "s-3bxJRvnCM4"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment was conducted to evaluate the methods across $10$ windows of size $0.3 * |D|$ with a $30\\%$ overlap throughout the multivariate time series of sample size $|D|$. The windows were divided into training and test sets."
      ],
      "metadata": {
        "id": "UdsUCROq1ceA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_text(df, name_dataset, target, max_lags, tokenizer):\n",
        "\n",
        "    variables = df.columns.tolist()\n",
        "    dict_variables = dict.fromkeys(variables)\n",
        "\n",
        "    # Causal graph generation\n",
        "    graph = feature_selection.causal_graph(df.head(2000), target=target, max_lags=max_lags)[target]\n",
        "    X, y_hat = util.organize_dataset(df, graph, max_lags, target)\n",
        "    y = df[target].squeeze().tolist()[max_lags:]\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    labels_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    inputs = fcllm.create_sequences_input(X, y, tokenizer)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    input_tokens = tokenizer(\n",
        "    inputs,\n",
        "    padding_side='left',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    #input_tokens = tokenizer(inputs, padding_side = 'left', padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    return fcllm.custom_Dataset(input_tokens.input_ids, input_tokens.attention_mask, labels_scaled), scaler, tokenizer, inputs\n",
        "\n",
        "def text(df, name_dataset, target, max_lags, tokenizer):\n",
        "\n",
        "    # Complete graph generation\n",
        "    graph = feature_selection.complete_graph(df.head(2000), target=target, max_lags=max_lags)[target]\n",
        "    X, y_hat = util.organize_dataset(df, graph, max_lags, target)\n",
        "    y = df[target].squeeze().tolist()[max_lags:]\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    labels_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    inputs = fcllm.create_sequences_input(X, y, tokenizer)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    #input_tokens = tokenizer(inputs, padding_side = 'left', padding=True, return_tensors=\"pt\")\n",
        "    input_tokens = tokenizer(\n",
        "    inputs,\n",
        "    padding_side='left',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return fcllm.custom_Dataset(input_tokens.input_ids, input_tokens.attention_mask, labels_scaled), scaler, tokenizer, inputs\n",
        "\n",
        "\n",
        "def fuzzy_causal(df, name_dataset, target, max_lags, tokenizer, partitions):\n",
        "\n",
        "    variables = df.columns.tolist()\n",
        "    dict_variables = dict.fromkeys(variables)\n",
        "\n",
        "    # Fuzzification of time series\n",
        "    data_fuzzy = pd.DataFrame(columns=variables)\n",
        "    for v in variables:\n",
        "        dict_variables[v] = fcllm.fuzzification(pd.DataFrame(df[v]), name_dataset, v, partitions, None)\n",
        "        data_fuzzy[v] = dict_variables[v][0]\n",
        "\n",
        "    # Causal graph generation\n",
        "    graph = feature_selection.causal_graph(df.head(2000), target=target, max_lags=max_lags)[target]\n",
        "    X, y_hat = util.organize_dataset(data_fuzzy, graph, max_lags, target)\n",
        "    y = dict_variables[target][1].squeeze().tolist()[max_lags:]\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    labels_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    inputs = fcllm.create_sequences_input(X, y, tokenizer)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    input_tokens = tokenizer(\n",
        "    inputs,\n",
        "    padding_side='left',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    #input_tokens = tokenizer(inputs, padding_side = 'left', padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    return fcllm.custom_Dataset(input_tokens.input_ids, input_tokens.attention_mask, labels_scaled), scaler, tokenizer, inputs"
      ],
      "metadata": {
        "id": "SSfB681LO1PL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the windows\n",
        "for i, window in enumerate(windows):\n",
        "      if i < 0:  # Enter the number of the last window saved in the database here\n",
        "        print(\"window already executed\")\n",
        "\n",
        "      else:\n",
        "\n",
        "        # Exclude constant series.\n",
        "        for variable in window.columns:\n",
        "            if window[variable].max() == window[variable].min():\n",
        "                window = window.drop(variable, axis=1)\n",
        "                print(f\"Variables {variable} were deleted because they are constant.\")\n",
        "\n",
        "        # Fuzzy-Causal\n",
        "        ds_F, scaler, tokenizer_F, seq_F = fuzzy_causal(window, name_dataset, target, max_lags, tokenizer, partitions)\n",
        "\n",
        "        # Text\n",
        "        ds_T, scaler, tokenizer_T, seq_T = text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        # Text-Causal\n",
        "        ds_G, scaler, tokenizer_G, seq_G = causal_text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        # train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "        break\n",
        "        # freeze = True # Freeze GPT-2 weights\n",
        "        # model = fcllm.train_model(train_dataset, name_model, epochs, scaler, freeze, path_model = None)\n",
        "\n",
        "        # forecasts, real = fcllm.predict(test_dataset, model, tokenizer, target, scaler)\n",
        "\n",
        "        # for j in range(len(forecasts)):\n",
        "        #     sd.execute_insert(\"INSERT INTO results VALUES(?, ?, ?, ?)\", (name_dataset, i, forecasts[j], real[j]), database_path)\n",
        "\n",
        "        # print(f'Save window: {i}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T23:37:20.531157Z",
          "iopub.execute_input": "2025-05-04T23:37:20.531458Z",
          "iopub.status.idle": "2025-05-05T00:40:52.065306Z",
          "shell.execute_reply.started": "2025-05-04T23:37:20.531438Z",
          "shell.execute_reply": "2025-05-05T00:40:52.064369Z"
        },
        "id": "WbZBWFCbnCM5"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ds_F))\n",
        "print(dir(ds_F))  # List available methods/attributes\n",
        "print(ds_F.__getitem__(0))  # Try accessing the first item manually"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWQodVELpTfR",
        "outputId": "a18cf995-1190-41ad-d980-284070c519d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fuzzy_causal_text.custom_Dataset'>\n",
            "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'attention_mask', 'input_ids', 'labels']\n",
            "{'input_ids': tensor([50256, 50256, 50256, 50256, 50256,    58, 26125,    15, 13872,    15,\n",
            "         3334,    15, 13872,    15,    60,   657,    13, 41322, 26704, 50256]), 'attention_mask': tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([-1.1405])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##name_dataset = 'CLIMATIC_1', target = 'glo_avg'"
      ],
      "metadata": {
        "id": "D4AHxICv4kDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer_F, tokenizer_G, tokenizer_T are the same (GPT-2 tokenizer)\n",
        "# and ds_F, ds_G, ds_T, seq_F, seq_G, seq_T are available from your code\n",
        "\n",
        "# Function to calculate sizes\n",
        "def calculate_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Token size (total and average, with and without padding)\n",
        "    total_tokens = sum(len(ds[i]['input_ids']) for i in range(len(ds)))\n",
        "    total_non_padding_tokens = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in range(len(ds))\n",
        "    )\n",
        "    avg_tokens = total_tokens / len(ds)\n",
        "    avg_non_padding_tokens = total_non_padding_tokens / len(ds)\n",
        "\n",
        "    # Text size (total and average character length)\n",
        "    total_chars = sum(len(text) for text in seq)\n",
        "    avg_chars = total_chars / len(seq)\n",
        "\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    print(f\"Dataset length (number of samples): {len(ds)}\")\n",
        "    print(f\"Total tokens (including padding): {total_tokens}\")\n",
        "    print(f\"Total non-padding tokens: {total_non_padding_tokens}\")\n",
        "    print(f\"Average tokens per sequence (including padding): {avg_tokens:.2f}\")\n",
        "    print(f\"Average non-padding tokens per sequence: {avg_non_padding_tokens:.2f}\")\n",
        "    print(f\"Total text size (characters): {total_chars}\")\n",
        "    print(f\"Average text size per sequence (characters): {avg_chars:.2f}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_sizes(ds_G, seq_G, tokenizer_G, \"GF-LLM (causal_text)\")\n",
        "calculate_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkyajuvuAD1r",
        "outputId": "eefe6c1a-74bc-40af-9580-4e4f9e6534ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CGF-LLM (fuzzy_causal):\n",
            "Dataset length (number of samples): 10479\n",
            "Total tokens (including padding): 461076\n",
            "Total non-padding tokens: 443109\n",
            "Average tokens per sequence (including padding): 44.00\n",
            "Average non-padding tokens per sequence: 42.29\n",
            "Total text size (characters): 786258\n",
            "Average text size per sequence (characters): 75.03\n",
            "\n",
            "GF-LLM (causal_text):\n",
            "Dataset length (number of samples): 10480\n",
            "Total tokens (including padding): 576400\n",
            "Total non-padding tokens: 298478\n",
            "Average tokens per sequence (including padding): 55.00\n",
            "Average non-padding tokens per sequence: 28.48\n",
            "Total text size (characters): 660437\n",
            "Average text size per sequence (characters): 63.02\n",
            "\n",
            "LLM (text):\n",
            "Dataset length (number of samples): 10480\n",
            "Total tokens (including padding): 10731520\n",
            "Total non-padding tokens: 10730875\n",
            "Average tokens per sequence (including padding): 1024.00\n",
            "Average non-padding tokens per sequence: 1023.94\n",
            "Total text size (characters): 29934389\n",
            "Average text size per sequence (characters): 2856.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_train_test_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Split the dataset into train (80%) and test (20%)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "    # Token counts (including and excluding padding)\n",
        "    train_token_count = sum(len(ds[i]['input_ids']) for i in train_dataset.indices)\n",
        "    train_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in train_dataset.indices\n",
        "    )\n",
        "    test_token_count = sum(len(ds[i]['input_ids']) for i in test_dataset.indices)\n",
        "    test_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in test_dataset.indices\n",
        "    )\n",
        "\n",
        "    # Text size (character length)\n",
        "    train_texts, test_texts = torch.utils.data.random_split(seq, [0.80, 0.20])\n",
        "    train_char_count = sum(len(text) for text in train_texts)\n",
        "    test_char_count = sum(len(text) for text in test_texts)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nTrain/Test Split for {method_name}:\")\n",
        "    print(f\"Train tokens (including padding): {train_token_count}\")\n",
        "    print(f\"Train non-padding tokens: {train_non_padding_token_count}\")\n",
        "    print(f\"Test tokens (including padding): {test_token_count}\")\n",
        "    print(f\"Test non-padding tokens: {test_non_padding_token_count}\")\n",
        "    print(f\"Train text size (characters): {train_char_count}\")\n",
        "    print(f\"Test text size (characters): {test_char_count}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_train_test_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_train_test_sizes(ds_G, seq_G, tokenizer_G, \"GF-LLM (causal_text)\")\n",
        "calculate_train_test_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QmLZc8MQiMc",
        "outputId": "859ee6ed-6b50-4bd8-f876-81189544f4b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train/Test Split for CGF-LLM (fuzzy_causal):\n",
            "Train tokens (including padding): 368896\n",
            "Train non-padding tokens: 354492\n",
            "Test tokens (including padding): 92180\n",
            "Test non-padding tokens: 88617\n",
            "Train text size (characters): 629000\n",
            "Test text size (characters): 157258\n",
            "\n",
            "Train/Test Split for GF-LLM (causal_text):\n",
            "Train tokens (including padding): 461120\n",
            "Train non-padding tokens: 238072\n",
            "Test tokens (including padding): 115280\n",
            "Test non-padding tokens: 60406\n",
            "Train text size (characters): 528553\n",
            "Test text size (characters): 131884\n",
            "\n",
            "Train/Test Split for LLM (text):\n",
            "Train tokens (including padding): 8585216\n",
            "Train non-padding tokens: 8584808\n",
            "Test tokens (including padding): 2146304\n",
            "Test non-padding tokens: 2146067\n",
            "Train text size (characters): 23949302\n",
            "Test text size (characters): 5985087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## name_dataset = 'Ebergy', target = 'power'"
      ],
      "metadata": {
        "id": "kGtTf25w4uZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer_F, tokenizer_G, tokenizer_T are the same (GPT-2 tokenizer)\n",
        "# and ds_F, ds_G, ds_T, seq_F, seq_G, seq_T are available from your code\n",
        "\n",
        "# Function to calculate sizes\n",
        "def calculate_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Token size (total and average, with and without padding)\n",
        "    total_tokens = sum(len(ds[i]['input_ids']) for i in range(len(ds)))\n",
        "    total_non_padding_tokens = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in range(len(ds))\n",
        "    )\n",
        "    avg_tokens = total_tokens / len(ds)\n",
        "    avg_non_padding_tokens = total_non_padding_tokens / len(ds)\n",
        "\n",
        "    # Text size (total and average character length)\n",
        "    total_chars = sum(len(text) for text in seq)\n",
        "    avg_chars = total_chars / len(seq)\n",
        "\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    print(f\"Dataset length (number of samples): {len(ds)}\")\n",
        "    print(f\"Total tokens (including padding): {total_tokens}\")\n",
        "    print(f\"Total non-padding tokens: {total_non_padding_tokens}\")\n",
        "    print(f\"Average tokens per sequence (including padding): {avg_tokens:.2f}\")\n",
        "    print(f\"Average non-padding tokens per sequence: {avg_non_padding_tokens:.2f}\")\n",
        "    print(f\"Total text size (characters): {total_chars}\")\n",
        "    print(f\"Average text size per sequence (characters): {avg_chars:.2f}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSsiJw8V5Eto",
        "outputId": "fe3539e7-0f60-4d43-d278-5d990e5c535a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CGF-LLM (fuzzy_causal):\n",
            "Dataset length (number of samples): 13119\n",
            "Total tokens (including padding): 380451\n",
            "Total non-padding tokens: 353012\n",
            "Average tokens per sequence (including padding): 29.00\n",
            "Average non-padding tokens per sequence: 26.91\n",
            "Total text size (characters): 1276952\n",
            "Average text size per sequence (characters): 97.34\n",
            "\n",
            "CG-LLM (causal_text):\n",
            "Dataset length (number of samples): 13120\n",
            "Total tokens (including padding): 839680\n",
            "Total non-padding tokens: 604276\n",
            "Average tokens per sequence (including padding): 64.00\n",
            "Average non-padding tokens per sequence: 46.06\n",
            "Total text size (characters): 1190523\n",
            "Average text size per sequence (characters): 90.74\n",
            "\n",
            "LLM (text):\n",
            "Dataset length (number of samples): 13120\n",
            "Total tokens (including padding): 13434880\n",
            "Total non-padding tokens: 13432451\n",
            "Average tokens per sequence (including padding): 1024.00\n",
            "Average non-padding tokens per sequence: 1023.81\n",
            "Total text size (characters): 23194823\n",
            "Average text size per sequence (characters): 1767.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_train_test_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Split the dataset into train (80%) and test (20%)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "    # Token counts (including and excluding padding)\n",
        "    train_token_count = sum(len(ds[i]['input_ids']) for i in train_dataset.indices)\n",
        "    train_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in train_dataset.indices\n",
        "    )\n",
        "    test_token_count = sum(len(ds[i]['input_ids']) for i in test_dataset.indices)\n",
        "    test_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in test_dataset.indices\n",
        "    )\n",
        "\n",
        "    # Text size (character length)\n",
        "    train_texts, test_texts = torch.utils.data.random_split(seq, [0.80, 0.20])\n",
        "    train_char_count = sum(len(text) for text in train_texts)\n",
        "    test_char_count = sum(len(text) for text in test_texts)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nTrain/Test Split for {method_name}:\")\n",
        "    print(f\"Train tokens (including padding): {train_token_count}\")\n",
        "    print(f\"Train non-padding tokens: {train_non_padding_token_count}\")\n",
        "    print(f\"Test tokens (including padding): {test_token_count}\")\n",
        "    print(f\"Test non-padding tokens: {test_non_padding_token_count}\")\n",
        "    print(f\"Train text size (characters): {train_char_count}\")\n",
        "    print(f\"Test text size (characters): {test_char_count}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_train_test_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_train_test_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_train_test_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaJdhZrK5H0O",
        "outputId": "3d86db17-9895-4166-fbf5-8cd64ecc1ef1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train/Test Split for CGF-LLM (fuzzy_causal):\n",
            "Train tokens (including padding): 304384\n",
            "Train non-padding tokens: 282428\n",
            "Test tokens (including padding): 76067\n",
            "Test non-padding tokens: 70584\n",
            "Train text size (characters): 1021690\n",
            "Test text size (characters): 255262\n",
            "\n",
            "Train/Test Split for CG-LLM (causal_text):\n",
            "Train tokens (including padding): 671744\n",
            "Train non-padding tokens: 483461\n",
            "Test tokens (including padding): 167936\n",
            "Test non-padding tokens: 120815\n",
            "Train text size (characters): 952364\n",
            "Test text size (characters): 238159\n",
            "\n",
            "Train/Test Split for LLM (text):\n",
            "Train tokens (including padding): 10747904\n",
            "Train non-padding tokens: 10745699\n",
            "Test tokens (including padding): 2686976\n",
            "Test non-padding tokens: 2686752\n",
            "Train text size (characters): 18564729\n",
            "Test text size (characters): 4630094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOT data"
      ],
      "metadata": {
        "id": "DdVn3974HPqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer_F, tokenizer_G, tokenizer_T are the same (GPT-2 tokenizer)\n",
        "# and ds_F, ds_G, ds_T, seq_F, seq_G, seq_T are available from your code\n",
        "\n",
        "# Function to calculate sizes\n",
        "def calculate_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Token size (total and average, with and without padding)\n",
        "    total_tokens = sum(len(ds[i]['input_ids']) for i in range(len(ds)))\n",
        "    total_non_padding_tokens = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in range(len(ds))\n",
        "    )\n",
        "    avg_tokens = total_tokens / len(ds)\n",
        "    avg_non_padding_tokens = total_non_padding_tokens / len(ds)\n",
        "\n",
        "    # Text size (total and average character length)\n",
        "    total_chars = sum(len(text) for text in seq)\n",
        "    avg_chars = total_chars / len(seq)\n",
        "\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    print(f\"Dataset length (number of samples): {len(ds)}\")\n",
        "    print(f\"Total tokens (including padding): {total_tokens}\")\n",
        "    print(f\"Total non-padding tokens: {total_non_padding_tokens}\")\n",
        "    print(f\"Average tokens per sequence (including padding): {avg_tokens:.2f}\")\n",
        "    print(f\"Average non-padding tokens per sequence: {avg_non_padding_tokens:.2f}\")\n",
        "    print(f\"Total text size (characters): {total_chars}\")\n",
        "    print(f\"Average text size per sequence (characters): {avg_chars:.2f}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APognhYuHT-L",
        "outputId": "e8994061-880f-4a71-8441-f392f4175873"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CGF-LLM (fuzzy_causal):\n",
            "Dataset length (number of samples): 29979\n",
            "Total tokens (including padding): 839412\n",
            "Total non-padding tokens: 781784\n",
            "Average tokens per sequence (including padding): 28.00\n",
            "Average non-padding tokens per sequence: 26.08\n",
            "Total text size (characters): 3147800\n",
            "Average text size per sequence (characters): 105.00\n",
            "\n",
            "CG-LLM (causal_text):\n",
            "Dataset length (number of samples): 29980\n",
            "Total tokens (including padding): 1738840\n",
            "Total non-padding tokens: 1012191\n",
            "Average tokens per sequence (including padding): 58.00\n",
            "Average non-padding tokens per sequence: 33.76\n",
            "Total text size (characters): 2064031\n",
            "Average text size per sequence (characters): 68.85\n",
            "\n",
            "LLM (text):\n",
            "Dataset length (number of samples): 29980\n",
            "Total tokens (including padding): 30699520\n",
            "Total non-padding tokens: 30699520\n",
            "Average tokens per sequence (including padding): 1024.00\n",
            "Average non-padding tokens per sequence: 1024.00\n",
            "Total text size (characters): 94793482\n",
            "Average text size per sequence (characters): 3161.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_train_test_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Split the dataset into train (80%) and test (20%)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "    # Token counts (including and excluding padding)\n",
        "    train_token_count = sum(len(ds[i]['input_ids']) for i in train_dataset.indices)\n",
        "    train_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in train_dataset.indices\n",
        "    )\n",
        "    test_token_count = sum(len(ds[i]['input_ids']) for i in test_dataset.indices)\n",
        "    test_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in test_dataset.indices\n",
        "    )\n",
        "\n",
        "    # Text size (character length)\n",
        "    train_texts, test_texts = torch.utils.data.random_split(seq, [0.80, 0.20])\n",
        "    train_char_count = sum(len(text) for text in train_texts)\n",
        "    test_char_count = sum(len(text) for text in test_texts)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nTrain/Test Split for {method_name}:\")\n",
        "    print(f\"Train tokens (including padding): {train_token_count}\")\n",
        "    print(f\"Train non-padding tokens: {train_non_padding_token_count}\")\n",
        "    print(f\"Test tokens (including padding): {test_token_count}\")\n",
        "    print(f\"Test non-padding tokens: {test_non_padding_token_count}\")\n",
        "    print(f\"Train text size (characters): {train_char_count}\")\n",
        "    print(f\"Test text size (characters): {test_char_count}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_train_test_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_train_test_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_train_test_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doJSi6sNHVBu",
        "outputId": "21cffbd1-3ee1-4e3f-8b50-00afe4169e11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train/Test Split for CGF-LLM (fuzzy_causal):\n",
            "Train tokens (including padding): 671552\n",
            "Train non-padding tokens: 625443\n",
            "Test tokens (including padding): 167860\n",
            "Test non-padding tokens: 156341\n",
            "Train text size (characters): 2518273\n",
            "Test text size (characters): 629527\n",
            "\n",
            "Train/Test Split for CG-LLM (causal_text):\n",
            "Train tokens (including padding): 1391072\n",
            "Train non-padding tokens: 810007\n",
            "Test tokens (including padding): 347768\n",
            "Test non-padding tokens: 202184\n",
            "Train text size (characters): 1651208\n",
            "Test text size (characters): 412823\n",
            "\n",
            "Train/Test Split for LLM (text):\n",
            "Train tokens (including padding): 24559616\n",
            "Train non-padding tokens: 24559616\n",
            "Test tokens (including padding): 6139904\n",
            "Test non-padding tokens: 6139904\n",
            "Train text size (characters): 75825454\n",
            "Test text size (characters): 18968028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Economics"
      ],
      "metadata": {
        "id": "dVfxD0jRSVhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer_F, tokenizer_G, tokenizer_T are the same (GPT-2 tokenizer)\n",
        "# and ds_F, ds_G, ds_T, seq_F, seq_G, seq_T are available from your code\n",
        "\n",
        "# Function to calculate sizes\n",
        "def calculate_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Token size (total and average, with and without padding)\n",
        "    total_tokens = sum(len(ds[i]['input_ids']) for i in range(len(ds)))\n",
        "    total_non_padding_tokens = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in range(len(ds))\n",
        "    )\n",
        "    avg_tokens = total_tokens / len(ds)\n",
        "    avg_non_padding_tokens = total_non_padding_tokens / len(ds)\n",
        "\n",
        "    # Text size (total and average character length)\n",
        "    total_chars = sum(len(text) for text in seq)\n",
        "    avg_chars = total_chars / len(seq)\n",
        "\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    print(f\"Dataset length (number of samples): {len(ds)}\")\n",
        "    print(f\"Total tokens (including padding): {total_tokens}\")\n",
        "    print(f\"Total non-padding tokens: {total_non_padding_tokens}\")\n",
        "    print(f\"Average tokens per sequence (including padding): {avg_tokens:.2f}\")\n",
        "    print(f\"Average non-padding tokens per sequence: {avg_non_padding_tokens:.2f}\")\n",
        "    print(f\"Total text size (characters): {total_chars}\")\n",
        "    print(f\"Average text size per sequence (characters): {avg_chars:.2f}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyGWhuCtSHS3",
        "outputId": "e5d3cfbe-ccf7-40e5-cfd7-00228c389535"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CGF-LLM (fuzzy_causal):\n",
            "Dataset length (number of samples): 867\n",
            "Total tokens (including padding): 17340\n",
            "Total non-padding tokens: 12388\n",
            "Average tokens per sequence (including padding): 20.00\n",
            "Average non-padding tokens per sequence: 14.29\n",
            "Total text size (characters): 43875\n",
            "Average text size per sequence (characters): 50.61\n",
            "\n",
            "CG-LLM (causal_text):\n",
            "Dataset length (number of samples): 868\n",
            "Total tokens (including padding): 32984\n",
            "Total non-padding tokens: 17768\n",
            "Average tokens per sequence (including padding): 38.00\n",
            "Average non-padding tokens per sequence: 20.47\n",
            "Total text size (characters): 40643\n",
            "Average text size per sequence (characters): 46.82\n",
            "\n",
            "LLM (text):\n",
            "Dataset length (number of samples): 868\n",
            "Total tokens (including padding): 846300\n",
            "Total non-padding tokens: 762734\n",
            "Average tokens per sequence (including padding): 975.00\n",
            "Average non-padding tokens per sequence: 878.73\n",
            "Total text size (characters): 1335281\n",
            "Average text size per sequence (characters): 1538.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_train_test_sizes(ds, seq, tokenizer, method_name):\n",
        "    # Split the dataset into train (80%) and test (20%)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "    # Token counts (including and excluding padding)\n",
        "    train_token_count = sum(len(ds[i]['input_ids']) for i in train_dataset.indices)\n",
        "    train_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in train_dataset.indices\n",
        "    )\n",
        "    test_token_count = sum(len(ds[i]['input_ids']) for i in test_dataset.indices)\n",
        "    test_non_padding_token_count = sum(\n",
        "        len([t for t in ds[i]['input_ids'] if t != tokenizer.pad_token_id])\n",
        "        for i in test_dataset.indices\n",
        "    )\n",
        "\n",
        "    # Text size (character length)\n",
        "    train_texts, test_texts = torch.utils.data.random_split(seq, [0.80, 0.20])\n",
        "    train_char_count = sum(len(text) for text in train_texts)\n",
        "    test_char_count = sum(len(text) for text in test_texts)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nTrain/Test Split for {method_name}:\")\n",
        "    print(f\"Train tokens (including padding): {train_token_count}\")\n",
        "    print(f\"Train non-padding tokens: {train_non_padding_token_count}\")\n",
        "    print(f\"Test tokens (including padding): {test_token_count}\")\n",
        "    print(f\"Test non-padding tokens: {test_non_padding_token_count}\")\n",
        "    print(f\"Train text size (characters): {train_char_count}\")\n",
        "    print(f\"Test text size (characters): {test_char_count}\")\n",
        "\n",
        "# Calculate for each method\n",
        "calculate_train_test_sizes(ds_F, seq_F, tokenizer_F, \"CGF-LLM (fuzzy_causal)\")\n",
        "calculate_train_test_sizes(ds_G, seq_G, tokenizer_G, \"CG-LLM (causal_text)\")\n",
        "calculate_train_test_sizes(ds_T, seq_T, tokenizer_T, \"LLM (text)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctdGE2fHSLS4",
        "outputId": "999186ce-f00a-4a99-8fd6-09d17239b473"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train/Test Split for CGF-LLM (fuzzy_causal):\n",
            "Train tokens (including padding): 13880\n",
            "Train non-padding tokens: 9950\n",
            "Test tokens (including padding): 3460\n",
            "Test non-padding tokens: 2438\n",
            "Train text size (characters): 35079\n",
            "Test text size (characters): 8796\n",
            "\n",
            "Train/Test Split for CG-LLM (causal_text):\n",
            "Train tokens (including padding): 26410\n",
            "Train non-padding tokens: 14256\n",
            "Test tokens (including padding): 6574\n",
            "Test non-padding tokens: 3512\n",
            "Train text size (characters): 32621\n",
            "Test text size (characters): 8022\n",
            "\n",
            "Train/Test Split for LLM (text):\n",
            "Train tokens (including padding): 677625\n",
            "Train non-padding tokens: 610872\n",
            "Test tokens (including padding): 168675\n",
            "Test non-padding tokens: 151862\n",
            "Train text size (characters): 1068135\n",
            "Test text size (characters): 267146\n"
          ]
        }
      ]
    }
  ]
}